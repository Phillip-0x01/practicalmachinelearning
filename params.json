{
  "name": "Practicalmachinelearning",
  "tagline": "Practical Machine Learning Course Project",
  "body": "---\r\ntitle: \"Machine Learning Course Project\"\r\nauthor: \"Phillip\"\r\ndate: \"March 14, 2016\"\r\noutput: html_document\r\n---\r\n\r\n\r\n\r\nThe following report was an assignment completed during my enrollment in the John Hopkin's Machine Learning Course on Coursera.  The Project Goal and Background Information sections below are copied directly from the course website while the rest of the paper details how I went about completing the assignment. \r\n\r\n\r\n\r\n###Project Goal\r\nThe goal of your project is to predict the manner in which they did the exercise. This is the “classe” variable in the training set. You may use any of the other variables to predict with. You should create a report describing how you built your model, how you used cross validation, what you think the expected out of sample error is, and why you made the choices you did. You will also use your prediction model to predict 20 different test cases.\r\n\r\nYour submission should consist of a link to a Github repo with your R markdown and compiled HTML file describing your analysis. Please constrain the text of the writeup to < 2000 words and the number of figures to be less than 5. It will make it easier for the graders if you submit a repo with a gh-pages branch so the HTML page can be viewed online (and you always want to make it easy on graders :-).\r\n\r\nYou should also apply your machine learning algorithm to the 20 test cases available in the test data above. Please submit your predictions in appropriate format to the programming assignment for automated grading. See the programming assignment for additional details.\r\n\r\n\r\n\r\n###Background Information\r\nUsing devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement – a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. \r\n\r\nIn this project, your goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website here: http://groupware.les.inf.puc-rio.br/har (see the section on the Weight Lifting Exercise Dataset).\r\n\r\n\r\n\r\n###Getting and Loading Datasets \r\nThe following anaylsis was conducted using R version 3.1.3 (2015-03-09) and a MacBook Pro (Retina, 15-inch, Mid 2014) with a 2.2 Ghz Intel Core i7 Processor and 16 GB 1600 MHz DDR3 Memory.  \r\n\r\nThe 'caret' and 'radomForest' packages will be used for this project so they are loaded into the working environment first.  I've also set a seed (1127) to be used for pseudo random number generation so others are able to reproduce the model below.  \r\n\r\nThe datasets were then downloaded from the websites listed above and stored in memory.  Alternatively, the datasets could have been downloaded to the hard drive and read in from there, however I chose not to do so for this project.  Since the test dataset is being used for course grading purposes the training dataset will need to be split 60/40 into a testing and training dataset to build the model.  After the data is split I'll print out the dimensions of both sets to review them.  \r\n \r\n \r\n```{r echo=TRUE}\r\nlibrary(caret)\r\nlibrary(randomForest)\r\n\r\nset.seed(1127)\r\n\r\ninTrainURL <- \"http://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv\"\r\ntestURL <- \"http://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv\"\r\n\r\ntrainData <- read.csv(url(inTrainURL), na.strings=c(\"NA\",\"#DIV/0!\",\"\"))\r\ncourseTesting <- read.csv(url(testURL), na.strings=c(\"NA\",\"#DIV/0!\",\"\"))\r\n\r\ninTrain <- createDataPartition(y=trainData$classe, p=0.6, list=FALSE)\r\ntraining <- trainData[inTrain,]\r\ntesting <- trainData[-inTrain,]\r\ndim(training); dim(testing)\r\n# rm(inTrain,trainData)\r\n```\r\n\r\n\r\n\r\n###Data Cleaning\r\nAs you can see above, the datasets contain 160 variables (less the one prediction variable) so before the model is created I'll want remove any of those which may have little or no impact.  I'll start by first looking at the number of na values per variable across the dataset to see if anything stands out.    \r\n\r\n\r\n```{r echo=TRUE}\r\nNAcounts <- apply(training, 2, function(x) length(which(!is.na(x))))\r\ntable(NAcounts)\r\n```\r\n\r\n\r\nThe table above shows there are 60 variables which have values for every record, while the rest of the variables have far less (i.e. a lot of na's), so I'll drop those across all three datasets. Next, I'll remove any variables that have near zero variance.  To do this I'm wrapping the function in an IF statement because if it returns zero variables with near zero variance the function will remove all of the variables from the datasets. \r\n\r\n\r\n```{r echo=TRUE}\r\ntraining <- training[ , colSums(is.na(training)) == 0]\r\nif (length(nearZeroVar(training)) > 0) {\r\n  training <- training[, -nearZeroVar(training)] \r\n}\r\n\r\ntesting <- testing[ ,colSums(is.na(testing)) ==0]\r\nif (length(nearZeroVar(testing)) > 0) {\r\n  testing <- testing[, -nearZeroVar(testing)] \r\n}\r\n\r\ncourseTesting <- courseTesting[ ,colSums(is.na(courseTesting)) ==0]\r\nif (length(nearZeroVar(courseTesting)) > 0) {\r\n  courseTesting <- courseTesting[, -nearZeroVar(courseTesting)] \r\n}\r\ndim(training); dim(testing)\r\n```\r\n\r\n\r\nWith 59 variables still in the datasets that means there was only one variable which had a near zero variance.  One last thing that I can do is look at the names of the variables and possible the raw data to determine if any others can be dropped.  This is usually something that would be done initially, however I was not able to find any documents which explained what each of the variables were and with 160 to go through I skipped that step.  \r\n\r\n\r\n```{r echo=TRUE}\r\nnames(training)\r\n```\r\n\r\n\r\nAfter reviewing the names and looking at the some of the raw data I've determine that the first six variables can be removed.  The first variable just contains row number, the second is the users name, and three through six contain data related to a timestamp.\r\n\r\n\r\n```{r echo=TRUE}\r\ntraining <- training[, -c(1:6)]  \r\ntesting <- testing[ ,-c(1:6)]\r\ncourseTesting <- courseTesting[ ,-c(1:6)]\r\n```\r\n\r\n\r\n###Model Training\r\nThe model is then created using the randomForest package with the number of tress set to the default value of 500.  Once done it's used to predict the classe variable in the testing dataset for cross validation purposes, where we'll look at the accuracy percentage to determine how well it performed.  \r\n\r\n\r\n```{r echo=TRUE}\r\nfitMod <- randomForest(classe ~ .,training, ntree=500)\r\ntesting$predicted.response <- predict(fitMod, testing)\r\nconfusionMatrix(data=testing$predicted.response, reference=testing$classe, positive='yes')\r\n```\r\n\r\n\r\nWow, 99.3% accuracy!  With an accuracy this high the out of sample error rate should be pretty low but let's double check to make sure. \r\n\r\n\r\n```{r echo=TRUE}\r\nmissClass = function(values, prediction) {\r\n  sum(prediction != values)/length(values)\r\n}\r\nmissClass(testing$classe, testing$predicted.response)\r\n```\r\n\r\n\r\nBased on the miss classification rate on the testing subset, an unbiased estimate of the random forest's out-of-sample error rate is 0.06%.\r\n\r\n\r\n###Quiz Prediction\r\n```{r echo=TRUE}\r\npredict(fitMod, courseTesting)\r\n```",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}